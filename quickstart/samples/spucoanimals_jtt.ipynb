{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_group = []\n",
    "minority_group = []\n",
    "for color in range(5):\n",
    "    for number in range(5):\n",
    "        if color == number:\n",
    "            majority_group = majority_group + trainset.true_group_partition[(color, number)]\n",
    "        else:\n",
    "            minority_group =  minority_group + trainset.true_group_partition[(color, number)]\n",
    "right_mi = 0\n",
    "precisions = []\n",
    "recalls = []\n",
    "for i in range(5):\n",
    "    first_group = (i,0)\n",
    "    second_group = (i,1)\n",
    "    if len(first_group) > len(second_group):\n",
    "        maj_group = first_group\n",
    "        min_group = second_group\n",
    "    else:\n",
    "        maj_group = second_group\n",
    "        min_group = first_group\n",
    "    for i in group_partition[min_group]:\n",
    "        if i in minority_group:\n",
    "            right_mi = right_mi + 1\n",
    "    precision = right_mi / (len(min_group) + 0.00001)\n",
    "    recall = right_mi / len(minority_group)\n",
    "    precisions.append(precision)\n",
    "    recall.append(recall)\n",
    "precision = np.mean(precisions)\n",
    "recalls = np.mean(recalls)\n",
    "ratios[str(seed)].append((precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.load(\"/home/hyang/SpuCo/quickstart/samples/jtt_analyze/epoch_0_lr_1e-05_wd_1.0_seed_0.npz\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0.936,\n",
       " (0, 1): 1.0,\n",
       " (1, 0): 0.92,\n",
       " (1, 1): 0.99,\n",
       " (2, 2): 1.0,\n",
       " (2, 3): 1.0,\n",
       " (3, 2): 0.0,\n",
       " (3, 3): 0.068}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['gd'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.optim import SGD\n",
    "from wilds import get_dataset\n",
    "\n",
    "from spuco.datasets import GroupLabeledDatasetWrapper, SpuCoAnimals\n",
    "from spuco.evaluate import Evaluator\n",
    "from spuco.group_inference import JTTInference\n",
    "from spuco.invariant_train import CustomSampleERM\n",
    "from spuco.models import model_factory\n",
    "from spuco.utils import Trainer, set_seed\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--gpu\", type=int, default=0)\n",
    "parser.add_argument(\"--seed\", type=int, default=0)\n",
    "parser.add_argument(\"--root_dir\", type=str, default=\"/data\")\n",
    "parser.add_argument(\"--label_noise\", type=float, default=0.0)\n",
    "parser.add_argument(\"--results_csv\", type=str, default=\"results/spucoanimals_jtt.csv\")\n",
    "\n",
    "parser.add_argument(\"--arch\", type=str, default=\"resnet18\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "parser.add_argument(\"--num_epochs\", type=int, default=100)\n",
    "parser.add_argument(\"--lr\", type=float, default=1e-4)\n",
    "parser.add_argument(\"--weight_decay\", type=float, default=1e-4)\n",
    "parser.add_argument(\"--momentum\", type=float, default=0.9)\n",
    "parser.add_argument(\"--pretrained\", action=\"store_true\")\n",
    "\n",
    "parser.add_argument(\"--infer_num_epochs\", type=int, default=7)\n",
    "\n",
    "parser.add_argument(\"--upsample_factor\", type=int, default=100)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "device = torch.device(f\"cuda:{args.gpu}\" if torch.cuda.is_available() else \"cpu\")\n",
    "set_seed(args.seed)\n",
    "\n",
    "# Load the full dataset, and download it if necessary\n",
    "transform = transforms.Compose([\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        ])\n",
    "\n",
    "# trainset = SpuCoAnimals(\n",
    "#     root=args.root_dir,\n",
    "#     label_noise=args.label_noise,\n",
    "#     split=\"train\",\n",
    "#     transform=transform,\n",
    "# )\n",
    "# trainset.initialize()\n",
    "\n",
    "valset = SpuCoAnimals(\n",
    "    root=args.root_dir,\n",
    "    label_noise=args.label_noise,\n",
    "    split=\"val\",\n",
    "    transform=transform,\n",
    ")\n",
    "valset.initialize()\n",
    "\n",
    "# testset = SpuCoAnimals(\n",
    "#     root=args.root_dir,\n",
    "#     label_noise=args.label_noise,\n",
    "#     split=\"test\",\n",
    "#     transform=transform,\n",
    "# )\n",
    "# testset.initialize()\n",
    "\n",
    "model = model_factory(args.arch, trainset[0][0].shape, trainset.num_classes, pretrained=args.pretrained).to(device)\n",
    "# os.makedirs(\"models/spucoanimals\", exist_ok=True)\n",
    "# torch.save(model.state_dict(), f\"models/spucoanimals/jtt_lr={args.lr}_wd={args.weight_decay}_seed={args.seed}_upsample_factor={args.upsample_factor}.pt\")\n",
    "\n",
    "\n",
    "# if not args.pretrained and args.infer_num_epochs < 0:\n",
    "#     max_f1 = 0\n",
    "#     logits_files = glob(f\"logits/spucoanimals/lr=0.001_wd=0.0001_seed={args.seed}/valset*.pt\")\n",
    "#     for logits_file in logits_files:\n",
    "#         epoch = int(logits_file.split(\"/\")[-1].split(\".\")[0].split(\"_\")[-1])\n",
    "#         if epoch >= args.num_epochs:\n",
    "#             continue\n",
    "#         logits = torch.load(logits_file)\n",
    "#         predictions = torch.argmax(logits, dim=-1).detach().cpu().tolist()\n",
    "#         jtt = JTTInference(\n",
    "#             predictions=predictions,\n",
    "#             class_labels=valset.labels\n",
    "#         )\n",
    "#         epoch_group_partition = jtt.infer_groups()\n",
    "\n",
    "#         upsampled_indices = epoch_group_partition[(0,1)]\n",
    "#         minority_indices = valset.group_partition[(0,1)]\n",
    "#         minority_indices.extend(valset.group_partition[(1,0)])\n",
    "#         # compute F1 score on the validation set\n",
    "#         upsampled = np.zeros(len(predictions))\n",
    "#         upsampled[np.array(upsampled_indices)] = 1\n",
    "#         minority = np.zeros(len(predictions))\n",
    "#         minority[np.array(minority_indices)] = 1\n",
    "#         f1 = f1_score(minority, upsampled)\n",
    "#         if f1 > max_f1:\n",
    "#             max_f1 = f1\n",
    "#             args.infer_num_epochs = epoch\n",
    "#             group_partition = epoch_group_partition\n",
    "#             print(\"New best F1 score:\", f1, \"at epoch\", epoch)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    trainset=valset,\n",
    "    model=model,\n",
    "    batch_size=args.batch_size,\n",
    "    optimizer=SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=args.momentum),\n",
    "    device=device,\n",
    "    verbose=True\n",
    ")\n",
    "trainer.train(num_epochs=args.infer_num_epochs)\n",
    "\n",
    "predictions = torch.argmax(trainer.get_trainset_outputs(), dim=-1).detach().cpu().tolist()\n",
    "jtt = JTTInference(\n",
    "    predictions=predictions,\n",
    "    class_labels=trainset.labels\n",
    ")\n",
    "\n",
    "group_partition = jtt.infer_groups()\n",
    "\n",
    "for key in sorted(group_partition.keys()):\n",
    "    print(key, len(group_partition[key]))\n",
    "# evaluator = Evaluator(\n",
    "#     testset=trainset,\n",
    "#     group_partition=group_partition,\n",
    "#     group_weights=trainset.group_weights,\n",
    "#     batch_size=args.batch_size,\n",
    "#     model=model,\n",
    "#     device=device,\n",
    "#     verbose=True\n",
    "# )\n",
    "# evaluator.evaluate()\n",
    "\n",
    "# invariant_trainset = GroupLabeledDatasetWrapper(trainset, group_partition)\n",
    "\n",
    "# val_evaluator = Evaluator(\n",
    "#     testset=testset,\n",
    "#     group_partition=testset.group_partition,\n",
    "#     group_weights=trainset.group_weights,\n",
    "#     batch_size=args.batch_size,\n",
    "#     model=model,\n",
    "#     device=device,\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# indices = []\n",
    "# indices.extend(group_partition[(0,0)])\n",
    "# indices.extend(group_partition[(0,1)] * args.upsample_factor)\n",
    "\n",
    "# print(\"Training on\", len(indices), \"samples\")\n",
    "\n",
    "# model = model_factory(args.arch, trainset[0][0].shape, trainset.num_classes, pretrained=args.pretrained).to(device)\n",
    "# jtt_train = CustomSampleERM(\n",
    "#     model=model,\n",
    "#     num_epochs=args.num_epochs,\n",
    "#     trainset=trainset,\n",
    "#     batch_size=args.batch_size,\n",
    "#     indices=indices,\n",
    "#     val_evaluator=val_evaluator,\n",
    "#     optimizer=SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=args.momentum),\n",
    "#     device=device,\n",
    "#     verbose=True\n",
    "# )\n",
    "# jtt_train.train()\n",
    "\n",
    "# # save the model\n",
    "# os.makedirs(\"models/spucoanimals\", exist_ok=True)\n",
    "# torch.save(model.state_dict(), f\"models/spucoanimals/jtt_lr={args.lr}_wd={args.weight_decay}_seed={args.seed}_upsample_factor={args.upsample_factor}.pt\")\n",
    "\n",
    "# # save the best model\n",
    "# torch.save(jtt_train.best_model.state_dict(), f\"models/spucoanimals/jtt_lr={args.lr}_wd={args.weight_decay}_seed={args.seed}_upsample_factor={args.upsample_factor}_best.pt\")\n",
    "\n",
    "# evaluator = Evaluator(\n",
    "#     testset=testset,\n",
    "#     group_partition=testset.group_partition,\n",
    "#     group_weights=trainset.group_weights,\n",
    "#     batch_size=args.batch_size,\n",
    "#     model=model,\n",
    "#     device=device,\n",
    "#     verbose=True\n",
    "# )\n",
    "# evaluator.evaluate()\n",
    "# results = pd.DataFrame(index=[0])\n",
    "# results[\"timestamp\"] = pd.Timestamp.now()\n",
    "# results[\"seed\"] = args.seed\n",
    "# results[\"pretrained\"] = args.pretrained\n",
    "# results[\"lr\"] = args.lr\n",
    "# results[\"weight_decay\"] = args.weight_decay\n",
    "# results[\"momentum\"] = args.momentum\n",
    "# results[\"num_epochs\"] = args.num_epochs\n",
    "# results[\"batch_size\"] = args.batch_size\n",
    "# results[\"upsample_factor\"] = args.upsample_factor\n",
    "# results[\"infer_num_epochs\"] = args.infer_num_epochs\n",
    "\n",
    "# results[\"worst_group_accuracy\"] = evaluator.worst_group_accuracy[1]\n",
    "# results[\"average_accuracy\"] = evaluator.average_accuracy\n",
    "\n",
    "# evaluator = Evaluator(\n",
    "#     testset=testset,\n",
    "#     group_partition=testset.group_partition,\n",
    "#     group_weights=trainset.group_weights,\n",
    "#     batch_size=args.batch_size,\n",
    "#     model=jtt_train.best_model,\n",
    "#     device=device,\n",
    "#     verbose=True\n",
    "# )\n",
    "# evaluator.evaluate()\n",
    "\n",
    "\n",
    "# results[\"early_stopping_worst_group_accuracy\"] = evaluator.worst_group_accuracy[1]\n",
    "# results[\"early_stopping_average_accuracy\"] = evaluator.average_accuracy\n",
    "\n",
    "# if os.path.exists(args.results_csv):\n",
    "#     results_df = pd.read_csv(args.results_csv)\n",
    "# else:\n",
    "#     results_df = pd.DataFrame()\n",
    "\n",
    "# results_df = pd.concat([results_df, results], ignore_index=True)\n",
    "# results_df.to_csv(args.results_csv, index=False)\n",
    "\n",
    "# print('Done!')\n",
    "# print('Results saved to', args.results_csv)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
